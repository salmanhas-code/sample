# sample
We have disabled auto-configuration for OpenAI chat to manually define both OpenAI and Ollama chat clients as beans. We use @Qualifier("openAiChatClient") to inject the specific bean into the controller.

For OpenAI, we need three properties:

api-key (authentication)
base-url (DeepSeek API endpoint)
model (DeepSeek reasoner model)
For Ollama, since it's running locally, we only need the model property.

1ï¸âƒ£ chatClient.prompt() â€“ Creating a Chat Prompt
This initializes a new chat prompt session.
The prompt() method provides a builder-like API to configure the request.

2ï¸âƒ£ .user(m) â€“ Adding the User's Message
.user(m) attaches the userâ€™s input (m) to the prompt.

ğŸ‘‰ This step tells the AI model what the user wants.

The message (m) can be a simple string like "Hello AI!" or more complex structured input.

3ï¸âƒ£ .call() â€“ Executing the API Request
The .call() method sends the request to the AI model for processing.

ğŸ‘‰ This step triggers the AI model to process the input and generate a response.

It interacts with OpenAI or Ollama, depending on which chat client was injected.

4ï¸âƒ£ .content() â€“ Extracting the AIâ€™s Response
The .call() method returns a ChatResponse object.
.content() extracts the AI-generated response as a plain string.

ğŸ‘‰ This step extracts the final response and returns it to the user.

1ï¸âƒ£ Start chat session â†’ 2ï¸âƒ£ Send user input â†’ 3ï¸âƒ£ Call the AI model â†’ 4ï¸âƒ£ Extract AIâ€™s response
